---
title: "rbmi: Advanced Functionality"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
    number_sections: true
    citation_package: natbib
bibliography: "references.bib"
vignette: >
  %\VignetteIndexEntry{advanced}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
)
```


# Introduction

The purpose of this vignette is to provide an overview of some of the more advanced
features of the `rbmi` package. 


In order to demonstrate these advanced functions we will first create a simulated dataset
and perform a mock analysis so that all the require objects are available.

```{r}
library(rbmi)
library(dplyr)
set.seed(169)
dat_full <- simulate_data(n = 120) %>% as_tibble()

## Introduce missingness
dat <- dat_full
missing_index_vis2 <- rbinom(nrow(dat), 1, 0.3) == 1 & dat$visit == "visit_2"
missing_index_vis3 <- rbinom(nrow(dat), 1, 0.4) == 1 & dat$visit == "visit_3"
dat[missing_index_vis2, "outcome"] <- NA_real_
dat[missing_index_vis3, "outcome"] <- NA_real_

# create data_ice setting the imputation method to MAR for
# each patient with at least one missing value
dat_ice <- dat %>%
    arrange(id, visit) %>%
    filter(is.na(outcome)) %>%
    group_by(id) %>%
    slice(1) %>%
    ungroup() %>%
    select(id, visit) %>%
    mutate(strategy = "MAR")


# Define the names of key variables in our dataset using `set_vars()`
# Note that covariates argument can contain interactions
vars <- set_vars(
    outcome = "outcome",
    visit = "visit",
    subjid = "id",
    group = "group",
    covariates = c("age", "sex", "group*visit")
)
```


# Efficiently Changing Imputation Strategies

The `draws()` function is by far the most computationally intensive function in `rbmi`. 
In some settings, it may be important to explore the impact of a different 
reference-based imputation strategy than the one chosen for the main analysis. 
This change in the imputation strategy does not affect the imputation model but it does 
affect the subsequent imputation step. 
In order to allow changes in the imputation strategy without having to re-run the 
`draws()` function, the function `impute()` has an additional argument `update_strategies`.  

Please note though that this functionality comes with a some key limitations:
From a theoretical point of view, such updates are only sensible if the base imputation model and the data to which it is fitted are also applicable to the revised imputation strategy. For example, as described in `help("draws", "rbmi")`, observed post-ICE data are included in the imputation model for the MAR strategy but excluded for non-MAR strategies. For this reason, the imputation strategy cannot be changed from MAR to a non-MAR strategy in the presence of observed data after the ICE. Likewise, a change from a non-MAR to MAR in the presence of observed data after the ICE triggers a warning because the imputation model was not fitted to all relevant data and may be inefficient. Similarly, it is not possible to change the first visit which is affected by an ICE via argument `update_strategies`. 

As an example, let's assume we want to use the copy  increments from reference strategy for the main analysis and explore the jump to reference imputation strategy as a sensitivity analysis. If we use approximate Bayesian multiple imputation with 10 random imputation, this could be implemented as follows:

```{r}
dat_ice_CIR <- dat_ice
dat_ice_CIR$strategy <- "CIR"
dat_ice_CIR

draw_obj <- draws(
    data = dat,
    data_ice = dat_ice_CIR,
    vars = vars,
    method = method_approxbayes(n_sample=10)
)

impute_obj_CIR <- impute(
    draw_obj,
    references = c("A" = "B", "B" = "B")
)

ana_obj_CIR <- analyse(
    impute_obj_CIR,
    vars = set_vars()
)

pool(ana_obj_CIR)

## Now we re-use our draws samples but using the Jump to Reference 
## Imputation strategy

dat_ice_JR <- dat_ice
dat_ice_JR$strategy <- "JR"
dat_ice_JR

impute_obj_JR <- impute(
    draw_obj,
    references = c("A" = "B", "B" = "B"),
    update_strategy = dat_ice_JR
)

ana_obj_JR <- analyse(
    impute_obj_JR,
    vars = set_vars()
)

pool(ana_obj_JR)
```


# Custom Imputation Strategies

The following imputation strategies are implemented in `rbmi`:

- Missing at Random (MAR)
- Jump to Reference (JR)
- Copy Reference (CR)
- Copy Increments from Reference (CIR)
- Last Mean Carried Forward (LMCF)

In addition, `rbmi` allows the user to implement their own imputation strategy. 
To do this there are three things which the user needs to do:

1. Define their own imputation strategy function. 
2. Specify which patients use this strategy in the `data_ice` dataset provided to `draws()`.
3. Provide the imputation strategy function to `impute()`.

The imputation strategy function must take 3 arguments (`pars_group`, `pars_ref`, and `index_mar`) and calculates the marginal mean trajectory and covariance matrix of a subject's marginal imputation distribution which will then be applied to subjects to which the strategy applies. 
Here, `pars_group` contains the predicted mean trajectory (`pars_group$mu`, a numeric vector) and covariance matrix (`pars_group$sigma`) for a subject conditional on their assigned treatment group and covariates. `pars_ref` contains the corresponding mean trajectory and covariance matrix conditional on the reference group and the subject's covariates. `index_mar` is a logical vector which specifies for each visit whether the visit is unaffected by an ICE handled using reference-based methods or not. As an example, the user can check how the CIR strategy was implemented by looking at function `strategy_CIR()`.

```{r}
strategy_CIR
```

To further illustrate this for a simple example, assume that a strategy is to be implemented according to which the marginal mean of the imputation distribution is equal to the marginal mean trajectory for the subject according to their assigned group and covariates up to the ICE and equal to the average of the visit-wise marginal means based on the subjects covariates and the assigned group or the reference group, respectively. For the covariance matrix of the marginal imputation distribution, the covariance matrix from the assigned group is taken. 


To do this, we first need to define the imputation function which for this example could be coded as: 

```{r}
strategy_AVG <- function(pars_group, pars_ref, index_mar) {
    mean_xy <- function(x, y) mean(c(x, y))
    mu_mean <- mapply(
        mean_xy,
        pars_group$mu,
        pars_ref$mu,
        SIMPLIFY = T
    )
    x <- pars_group
    x$mu[!index_mar] <- mu_mean[!index_mar]
    return(x)
}
```

And an example showing its use:
```{r}
pars_group <- list(
    mu = c(1, 2, 3),
    sigma = as_vcov(c(1, 3, 2), c(0.4, 0.5, 0.45))
)

pars_ref <- list(
    mu = c(5, 6, 7),
    sigma = as_vcov(c(2, 1, 1), c(0.7, 0.8, 0.5))
)

index_mar <- c(TRUE, TRUE, FALSE)

strategy_AVG(pars_group, pars_ref, index_mar)
```

To incorporate this into `rbmi`, `data_ice` needs to be updated such that subjects are 
specified as using the `AVG` imputation strategy. Additionally, the function needs
to be provided to `impute()` via the `getStrategies()` function as shown below:

```{r}
dat_ice$strategy <- "AVG"
dat_ice

draw_obj <- draws(
    data = dat,
    data_ice = dat_ice,
    vars = vars,
    method = method_approxbayes(n_sample=10)
)

impute_obj <- impute(
    draw_obj,
    references = c("A" = "B", "B" = "B"),
    strategies = getStrategies(AVG = strategy_AVG)
)
```

# Custom Analysis Functions

By default `rbmi` will analyse the data by using the `ancova()` function as the analysis function which fits 
an ANCOVA model to the outcomes from each visit separately,
and returns the "treatment effect" estimate as well as the corresponding least square means
for each group. If the user wants to perform a different analysis, or return different
statistics from the analysis, then this can be done by using a custom analysis function.
Beware that for conditional mean imputation, the consistency of treatment effect estimation
has only been formally established for analysis functions corresponding to linear models (such as ANCOVA) and caution is 
required when applying alternative analysis functions.

The custom analysis function must take a `data.frame` as its 
first argument and return a named `list` with each element itself being a `list`
containing a at a minimum a point estimate, called `est`. 
For method `method_bayes()` or `method_approxbayes()`, the list must additionally contain a 
standard error (element `se`) and, if available, the degrees of freedom of the complete-data analysis model (element `df`). 

As an example, let's say that the analysis compares the proportion of subjects with an outcome > 10 at the last 
visit between the groups without any covariate adjustment. This could then lead to the following (naive) analysis function:

```{r}
compare_prop_lastvisit <- function(data,...){
  
  fit <- summary(
    glm(
      I(outcome > 10) ~ group,
      family = binomial(),
      data = data[data[["visit"]] == "visit_3", ]
    )
  )
  
  res <- list(
    trt = list( 
      est = fit$coefficients["groupB", "Estimate"],
      se = fit$coefficients["groupB", "Std. Error"],
      df = Inf
    )
  )
  
  return(res)
}
```

The user should be aware that according to Rubin's rules, if the degrees of freedom from the complete-data analysis model is infinite, this does not imply that the pooled degrees of freedom is also infinite. It can be proven that in this case the pooled degrees of freedom is `(M-1)/lambda^2`, where `M` is the number of imputations and `lambda` is the fraction of missing information. Please see @Barnard1999 for details. If you want to force the pooled degrees of freedom to be `Inf`, please set `df = NA` as returned value from the analysis function.

This analysis function can then be used in combination with `analyse()` as follows:

```{r}
anl_obj <- analyse(
    imputations = impute_obj_CIR,
    fun = compare_prop_lastvisit)

pool(anl_obj)
```

# Delta Adjustment

The `delta` argument of `analyse()` allows users to modify the outcome variable
which can be utilised as part of a tipping point or sensitivity analysis. To do 
this, the user needs to provide a `data.frame` containing a column for the subject
and visit which identifies the observation to be adjusted, and then a 3rd column
called `delta` which specifies the value which will be added to the outcome prior to the analysis. 

The `delta_template()` function creates a skeleton `data.frame` containing
1 row per subject per visit with the value of delta set to 0 for all observations.
The `delta_template()` function has two additional mandatory arguments `delta`
and `dlag` which allow the user to specify initial cumulative delta values based upon a
default value and a scaling coefficient based upon how far away the visit
in question is from the ICE visit. 

More specifically; the `delta` argument specifies the default amount of delta
that should be applied to each post-ICE visit, whilst
`dlag` specifies the scaling coefficient to be applied based upon the visits proximity
to first visit affected by the ICE. By default, the delta will only be added to unobserved (i.e. imputed) post-ICE 
outcomes but this can be changed by setting the optional argument `missing_only = FALSE`.

The usage of the `delta` and `dlag` arguments is best illustrated with a few examples:

Let `delta = c(5,6,7,8)` and `dlag=c(1,2,3,4)` (i.e. assuming there are 4 visits) and lets 
say that the subject's first visit affected by the ICE is visit 2. The calculation would then be as follows:

```
v1  v2  v3  v4
--------------
 5   6   7   8  # delta assigned to each visit
 0   1   2   3  # scaling starting from the first visit after the subjects ICE
--------------
 0   6  14  24  # delta * scaling
--------------
 0   6  20  44  # accumulative sum / delta to be applied to each visit
```

That is to say the subject would have a delta offset of 0 applied for visit v1, 6 for 
visit v2, 20 for visit v3 and 44 for visit v4. As a comparison, lets say that the subject 
instead that the subject's first visit affected by the ICE was visit 3. Then, would be as follows:

```
v1  v2  v3  v4
--------------
 5   6   7   8  # delta assigned to each visit
 0   0   1   2  # scaling starting from the first visit after the subjects ICE
--------------
 0   0   7  16  # delta * scaling
--------------
 0   0   7  23  # accumulative sum / delta to be applied to each visit
 ```
 
If one alternatively wanted to apply a constant delta value of 5 to all visits affected by the ICE
regarrdless of their proximity to the first ICE visit, this could be achieved
by setting `delta = c(5,5,5,5)` and `dlag = c(1,0,0,0)`. For example lets say the first visit v1 
was already affected by the ICE for a subject. Then the calculation would be as follows:

```
v1  v2  v3  v4
--------------
 5   5   5   5  # delta assigned to each visit
 1   0   0   0  # scaling starting from the first visit after the subjects ICE
--------------
 5   0   0  0  # delta * scaling
--------------
 5   5   5  5  # accumulative sum / delta to be applied to each visit
 ```

!!! I tried to clean up the wording accordingly but ran into some problems. Please help to fix this! 

Another way of using these arguments is to set delta to be the difference in time 
between visits and dlag to be the amount of delta per unit of time. For example 
lets say that we have a visit on weeks 1, 5, 6 & 9 and that we want a delta of 3
to be applied for each week after an ICE. This can be achieved by setting 
`delta = c(0,4,1,3)` (the difference in weeks between each visit) and `dlag = c(3/2, 3, 3, 3)`. 
For example subjects who experienced an ICE between visit v1 and v2 we might assume that the ICE happened at half way between the two visits, that is, at week 3. Then the first visit affected by the ICE would be visit v2 (i.e at week 5) and
the calculation would be:

```
v1  v2  v3  v4
--------------
 0   4   1   3  # delta assigned to each visit
 0   3/2   3   3  # scaling starting from the first visit after the subjects ICE 
--------------
 0   6   3   9  # delta * scaling
--------------
 0   0   3  12  # accumulative sum / delta to be applied to each visit
```

i.e. on week-6 (1 week after the ICE) they have a delta of 3 and on week-9 (4 weeks
after the ICE) they have a delta of 12


To show this in action, lets say that we want a constant delta of 5 to be applied (regardless of
the lag) to all unobserved post-ICE visits in the treatment arm only. This can be achieved as follows:

First use the `delta` and `dlag` arguments of `delta_template()` to setup a template `data.frame`
in which every unobserved post-ICE visit has a delta of 5:
```{r}
delta_df <- delta_template(
    impute_obj_CIR, 
    delta = c(5, 5, 5), 
    dlag = c(1, 0, 0)
)

as_tibble(delta_df)
```

Next we can use the additional metadata variables provided by `delta_template()` to  manually 
reset the delta values for the control group back to 0:
```{r}
delta_df2 <- delta_df %>% 
    mutate(delta = if_else(group == "B", 0, delta))

as_tibble(delta_df2)
```

Finally we can now use our delta `data.frame` to apply our desired delta offset to our analysis:
```{r}
anl_delta <- analyse(impute_obj_CIR, delta = delta_df2, vars = set_vars())
pool(anl_delta)  
```
